{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syntactic_similarity import syntactic_similarity_driver\n",
    "from structural_similarity import structural_similarity_driver\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating the scores csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scores_csv(df,path):\n",
    "    firstIndex = path.find(\"n_\")\n",
    "    finalIndex = path.find(\"_\",firstIndex+2)\n",
    "    n = int(path[firstIndex+2:finalIndex])\n",
    "\n",
    "    cols = [\n",
    "        'task_id', 'prompt', 'sequence_similarity','edit_distance_score',\n",
    "        'jaccard_similarity','cosine_similarity_score','sorensen_dice_coefficient',\n",
    "        'hamming_distance_score', 'longest_common_subsequence', 'UnifiedDiff','TreeDiff'\n",
    "    ] + [f'pass_rate_{i}' for i in range(n+1)]\n",
    "\n",
    "\n",
    "    final = pd.DataFrame(columns = cols)\n",
    "\n",
    "    #iterating over the rows of the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        #getting the reference and the generated text\n",
    "        generated_codes = list(row[[f'code_{i}' for i in range(n+1)]])\n",
    "            \n",
    "        #computing the structural similarity\n",
    "        _, structural_similarity_scores_dict,_ = structural_similarity_driver(generated_codes)\n",
    "        \n",
    "        #computing the syntactic similarity\n",
    "        _, syntactic_similarity_scores_dict,_ = syntactic_similarity_driver(generated_codes)\n",
    "        \n",
    "        #dict to store the data\n",
    "        dict = {'task_id': row['task_id'], 'prompt': row['prompt']}\n",
    "\n",
    "        for i in range(n+1):\n",
    "            dict['pass_rate_'+str(i)] = row['pass_rate_'+str(i)]\n",
    "            \n",
    "        #appending the syntactic similarity scores to the dict\n",
    "        for key in syntactic_similarity_scores_dict.keys():\n",
    "            dict[key] = syntactic_similarity_scores_dict[key]\n",
    "\n",
    "        #appending the structural similarity scores to the dict\n",
    "        for key in structural_similarity_scores_dict.keys():\n",
    "            dict[key] = structural_similarity_scores_dict[key]\n",
    "\n",
    "        temp = pd.DataFrame(dict, index=[0])\n",
    "\n",
    "        #concatenating the dataframes\n",
    "        final = pd.concat([final, temp], ignore_index=True)\n",
    "    \n",
    "    path = path.replace(\".csv\",\"_sim_score.csv\")\n",
    "    path = path.replace(\"RESULTS\",\"RESULTS/final scores\")\n",
    "    final.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35193/1187370557.py:43: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final = pd.concat([final, temp], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores generated for  ./RESULTS/dataset_HumanEval_model_gpt-4-turbo-preview_n_5_tempr_0_temps_1_trial_1.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '/RESULTS/stats.cs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[163], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[1;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mgenerate_scores_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScores generated for \u001b[39m\u001b[38;5;124m\"\u001b[39m, file)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScores generated successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[114], line 4\u001b[0m, in \u001b[0;36mgenerate_scores_csv\u001b[0;34m(df, path)\u001b[0m\n\u001b[1;32m      2\u001b[0m firstIndex \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m finalIndex \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m,firstIndex\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfirstIndex\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mfinalIndex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m cols \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence_similarity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medit_distance_score\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjaccard_similarity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine_similarity_score\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msorensen_dice_coefficient\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhamming_distance_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongest_common_subsequence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnifiedDiff\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTreeDiff\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m ] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpass_rate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     13\u001b[0m final \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m cols)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '/RESULTS/stats.cs'"
     ]
    }
   ],
   "source": [
    "# Get a list of all CSV files in the directory\n",
    "csv_files = glob.glob('./RESULTS/*.csv')\n",
    "\n",
    "# Loop over the files and read them into pandas DataFrames\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    generate_scores_csv(df,file)\n",
    "    print(\"Scores generated for \", file)\n",
    "\n",
    "print(\"Scores generated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(df,path):\n",
    "    print(\"Visualizing the scores of the file \",path)\n",
    "    firstIndex = path.find(\"n_\")\n",
    "    finalIndex = path.find(\"_\",firstIndex+2)\n",
    "    n = int(path[firstIndex+2:finalIndex])\n",
    "    # print(df.index)\n",
    "\n",
    "    df['avg_pass_rate'] = df[[f'pass_rate_{i}' for i in range(n+1)]].mean(axis=1)\n",
    "    #sort by avg pass rate\n",
    "    df = df.sort_values(by='avg_pass_rate',ascending=True)\n",
    "    #scoring columns\n",
    "   # scores_columns = ['sequence_similarity','edit_distance_score','jaccard_similarity','cosine_similarity_score','sorensen_dice_coefficient','hamming_distance_score','longest_common_subsequence','UnifiedDiff','TreeDiff']\n",
    "    scores_columns = ['sequence_similarity','UnifiedDiff']\n",
    "\n",
    "    #print the number of data points that has the avg pass rate of 100\n",
    "    print(\"number of data points that has the avg pass rate = 100 \",len(df[df['avg_pass_rate'] == 100] )/ len(df))\n",
    "\n",
    "    corrs=-2\n",
    "    file = \"\"\n",
    "    #plot the avg pass rate with the task id and a score metrics\n",
    "    for score in scores_columns:\n",
    "        print(\"The correlation between the average pass rate and\",score,\"is\",df['avg_pass_rate'].corr(df[score]))\n",
    "        if df['avg_pass_rate'].corr(df[score]) > corrs:\n",
    "            corrs = df['avg_pass_rate'].corr(df[score])\n",
    "            file = path\n",
    "\n",
    "        #plot the avg pass rate wih color red and sequence similarity with color blue\n",
    "        plt.figure(figsize=(25,10))\n",
    "        plt.scatter(df[\"task_id\"] ,df[score]*100,color='blue')\n",
    "        plt.scatter(df[\"task_id\"],df['avg_pass_rate'],color='red')\n",
    "        plt.xlabel('Task ID')\n",
    "        #rotate the x axis labels\n",
    "        plt.xticks(ticks = [i for i in range(len(df))],labels = [i for i in range(len(df))], rotation=90)\n",
    "        plt.ylabel('Scores')\n",
    "        plt.legend([score,'Average Pass Rate'])\n",
    "        display_path = path.split(\"/\")[-1]\n",
    "        display_path = display_path.replace(\"_trial_1_sim_score.csv\",\"\")\n",
    "        plt.title('Average Pass Rate vs ' + score + \" \\nConfig: \" + display_path)\n",
    "       # plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Visualizing the scores of the file \",path,\" completed\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    print(\"The file with the highest correlation between the average pass rate and a score metric is \",file,\" with a correlation of \",corrs)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stats(df,path):\n",
    "    print(\"generating stats of the file \",path)\n",
    "    firstIndex = path.find(\"n_\")\n",
    "    finalIndex = path.find(\"_\",firstIndex+2)\n",
    "    n = int(path[firstIndex+2:finalIndex])\n",
    "\n",
    "    df['avg_pass_rate'] = df[[f'pass_rate_{i}' for i in range(n+1)]].mean(axis=1)\n",
    "    #sort by avg pass rate\n",
    "    df = df.sort_values(by='avg_pass_rate',ascending=True)\n",
    "    #scoring columns\n",
    "    scores_columns = ['sequence_similarity','edit_distance_score','jaccard_similarity','cosine_similarity_score','sorensen_dice_coefficient','hamming_distance_score','longest_common_subsequence','UnifiedDiff','TreeDiff']\n",
    "    dict = {}\n",
    "    #plot the avg pass rate with the task id and a score metrics\n",
    "    for score in scores_columns:\n",
    "        dict[score] = df['avg_pass_rate'].corr(df[score]).round(2)\n",
    "    \n",
    "    return dict\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looping to get the visuals and stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing the scores of the file  ./RESULTS/trial 3\\dataset_HumanEval_model_gpt-3.5-turbo_n_10_tempr_0_temps_1_trial_3.csv\n",
      "number of data points that has the avg pass rate = 100  0.32926829268292684\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sequence_similarity'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sequence_similarity'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[0;32m     10\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file)\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mvisualize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m generate_stats(df,file)\n",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m, in \u001b[0;36mvisualize\u001b[1;34m(df, path)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#plot the avg pass rate with the task id and a score metrics\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m scores_columns:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe correlation between the average pass rate and\u001b[39m\u001b[38;5;124m\"\u001b[39m,score,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis\u001b[39m\u001b[38;5;124m\"\u001b[39m,df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_pass_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcorr(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_pass_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcorr(df[score]) \u001b[38;5;241m>\u001b[39m corrs:\n\u001b[0;32m     24\u001b[0m         corrs \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_pass_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcorr(df[score])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sequence_similarity'"
     ]
    }
   ],
   "source": [
    "# Get a list of all CSV files in the directory\n",
    "#csv_files = glob.glob('./RESULTS/final scores/*.csv')\n",
    "import glob\n",
    "import pandas as pd\n",
    "csv_files = glob.glob('./RESULTS/trial 3/*.csv')\n",
    "\n",
    "stats =pd.DataFrame()\n",
    "# Loop over the files and read them into pandas DataFrames\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    visualize(df,file)\n",
    "    \n",
    "    dict = {}\n",
    "\n",
    "    dict = generate_stats(df,file)\n",
    "    dict['file'] = file\n",
    "\n",
    "    #generate a dataframe from the dict\n",
    "    temp = pd.DataFrame(dict, index=[0])\n",
    "    #reorder the columns to have the file column as the first column\n",
    "    cols = temp.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    temp = temp[cols]\n",
    "    \n",
    "    stats = pd.concat([stats,temp], ignore_index=True)\n",
    "\n",
    "print(\"Scores generated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exporting the stats to CSV and Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = (stats.T)\n",
    "stats.to_csv(\"RESULTS/stats.csv\", index=True)\n",
    "stats.to_excel(\"RESULTS/stats.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <td>./RESULTS/final scores/dataset_HumanEval_model...</td>\n",
       "      <td>./RESULTS/final scores/dataset_HumanEval_model...</td>\n",
       "      <td>./RESULTS/final scores/dataset_HumanEval_model...</td>\n",
       "      <td>./RESULTS/final scores/dataset_HumanEval_model...</td>\n",
       "      <td>./RESULTS/final scores/dataset_HumanEval_model...</td>\n",
       "      <td>./RESULTS/final scores/dataset_HumanEval_model...</td>\n",
       "      <td>./RESULTS/final scores/dataset_HumanEval_model...</td>\n",
       "      <td>./RESULTS/final scores/dataset_HumanEval_model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence_similarity</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edit_distance_score</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_similarity</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosine_similarity_score</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sorensen_dice_coefficient</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hamming_distance_score</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longest_common_subsequence</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UnifiedDiff</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TreeDiff</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            0  \\\n",
       "file                        ./RESULTS/final scores/dataset_HumanEval_model...   \n",
       "sequence_similarity                                                      0.28   \n",
       "edit_distance_score                                                      0.27   \n",
       "jaccard_similarity                                                       0.23   \n",
       "cosine_similarity_score                                                  0.28   \n",
       "sorensen_dice_coefficient                                                 0.1   \n",
       "hamming_distance_score                                                   0.17   \n",
       "longest_common_subsequence                                               0.24   \n",
       "UnifiedDiff                                                               0.3   \n",
       "TreeDiff                                                                 0.28   \n",
       "\n",
       "                                                                            1  \\\n",
       "file                        ./RESULTS/final scores/dataset_HumanEval_model...   \n",
       "sequence_similarity                                                      0.28   \n",
       "edit_distance_score                                                      0.27   \n",
       "jaccard_similarity                                                       0.27   \n",
       "cosine_similarity_score                                                  0.26   \n",
       "sorensen_dice_coefficient                                                0.11   \n",
       "hamming_distance_score                                                   0.19   \n",
       "longest_common_subsequence                                               0.24   \n",
       "UnifiedDiff                                                              0.25   \n",
       "TreeDiff                                                                 0.24   \n",
       "\n",
       "                                                                            2  \\\n",
       "file                        ./RESULTS/final scores/dataset_HumanEval_model...   \n",
       "sequence_similarity                                                      0.23   \n",
       "edit_distance_score                                                      0.19   \n",
       "jaccard_similarity                                                       0.17   \n",
       "cosine_similarity_score                                                  0.16   \n",
       "sorensen_dice_coefficient                                                0.07   \n",
       "hamming_distance_score                                                   0.07   \n",
       "longest_common_subsequence                                               0.14   \n",
       "UnifiedDiff                                                               0.3   \n",
       "TreeDiff                                                                 0.22   \n",
       "\n",
       "                                                                            3  \\\n",
       "file                        ./RESULTS/final scores/dataset_HumanEval_model...   \n",
       "sequence_similarity                                                      0.47   \n",
       "edit_distance_score                                                      0.43   \n",
       "jaccard_similarity                                                        0.4   \n",
       "cosine_similarity_score                                                  0.43   \n",
       "sorensen_dice_coefficient                                                0.25   \n",
       "hamming_distance_score                                                    0.3   \n",
       "longest_common_subsequence                                               0.35   \n",
       "UnifiedDiff                                                              0.42   \n",
       "TreeDiff                                                                 0.37   \n",
       "\n",
       "                                                                            4  \\\n",
       "file                        ./RESULTS/final scores/dataset_HumanEval_model...   \n",
       "sequence_similarity                                                      0.35   \n",
       "edit_distance_score                                                      0.34   \n",
       "jaccard_similarity                                                       0.29   \n",
       "cosine_similarity_score                                                  0.24   \n",
       "sorensen_dice_coefficient                                                0.09   \n",
       "hamming_distance_score                                                   0.22   \n",
       "longest_common_subsequence                                               0.27   \n",
       "UnifiedDiff                                                              0.27   \n",
       "TreeDiff                                                                 0.23   \n",
       "\n",
       "                                                                            5  \\\n",
       "file                        ./RESULTS/final scores/dataset_HumanEval_model...   \n",
       "sequence_similarity                                                      0.09   \n",
       "edit_distance_score                                                      0.08   \n",
       "jaccard_similarity                                                       0.13   \n",
       "cosine_similarity_score                                                   0.1   \n",
       "sorensen_dice_coefficient                                               -0.03   \n",
       "hamming_distance_score                                                   0.08   \n",
       "longest_common_subsequence                                               0.08   \n",
       "UnifiedDiff                                                              0.15   \n",
       "TreeDiff                                                                 0.12   \n",
       "\n",
       "                                                                            6  \\\n",
       "file                        ./RESULTS/final scores/dataset_HumanEval_model...   \n",
       "sequence_similarity                                                      0.21   \n",
       "edit_distance_score                                                      0.19   \n",
       "jaccard_similarity                                                       0.16   \n",
       "cosine_similarity_score                                                  0.21   \n",
       "sorensen_dice_coefficient                                                0.05   \n",
       "hamming_distance_score                                                   0.08   \n",
       "longest_common_subsequence                                               0.14   \n",
       "UnifiedDiff                                                              0.28   \n",
       "TreeDiff                                                                 0.25   \n",
       "\n",
       "                                                                            7  \n",
       "file                        ./RESULTS/final scores/dataset_HumanEval_model...  \n",
       "sequence_similarity                                                       0.3  \n",
       "edit_distance_score                                                      0.28  \n",
       "jaccard_similarity                                                        0.3  \n",
       "cosine_similarity_score                                                   0.3  \n",
       "sorensen_dice_coefficient                                                0.19  \n",
       "hamming_distance_score                                                   0.18  \n",
       "longest_common_subsequence                                               0.26  \n",
       "UnifiedDiff                                                              0.33  \n",
       "TreeDiff                                                                 0.28  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
